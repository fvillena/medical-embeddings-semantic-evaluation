## This is a guidance for running the code on servers of Prof. Liu.
For the detail of the methodology, see the slides
 [Presentation Link](https://docs.google.com/presentation/d/1eyqICs6EJ0JALZkUqGJh5h2r3jt4aptCueWyutFYd98/edit?usp=sharing)

## Overview processings
1. Download Wikipedia (English) dump dataset from [https://dumps.wikimedia.org/enwiki/](https://dumps.wikimedia.org/enwiki/)
2. Extract texts from Wikipedia dataset using open source tool [wikiextractor](https://github.com/zwChan/wikiextractor)
3. Tokenize the texts from Wikipedia using class Word2vec in [Clinical-Text-Mining](https://github.com/zwChan/Clinical-Text-Mining).



